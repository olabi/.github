{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa7091a-4bd9-4e09-941e-2cdacec86f68",
   "metadata": {},
   "source": [
    "## Check Drivers and Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894d7dae-e4de-4f69-88e4-4edcd419a8b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/HDD/JeYoung/Fundamentals_of_GPU_DataScience_with_RAPIDS/Chapter07/pyspark/spark-3.4.4-bin-hadoop3\n",
      "/media/HDD/JeYoung/Fundamentals_of_GPU_DataScience_with_RAPIDS/Chapter07\n",
      "/media/HDD/JeYoung/Fundamentals_of_GPU_DataScience_with_RAPIDS/Chapter07/pyspark/rapids-4-spark_2.12-24.12.0.jar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = f\"{os.getcwd()}/pyspark/spark-3.4.4-bin-hadoop3\"\n",
    "os.environ[\"PATH\"] =f\"{os.environ.get('SPARK_HOME')}/bin:{os.environ.get('PATH')}\"\n",
    "os.environ[\"SPARK_JARS\"] = '/media/HDD/JeYoung/Fundamentals_of_GPU_DataScience_with_RAPIDS/Chapter07/pyspark/rapids-4-spark_2.12-24.12.0.jar'\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
    "\n",
    "# Set CUDA Visiblity\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "# If you use port 127.0.0.1(basic port), you don't need to set below codes\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"192.168.0.69\"\n",
    "\n",
    "\n",
    "# if you need to add JAVA path, run below code\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"PATH\"]=f\"{os.environ.get('JAVA_HOME')}/bin:{os.environ.get('PATH')}\"\n",
    "\n",
    "!echo $SPARK_HOME\n",
    "!echo $PWD\n",
    "!echo $SPARK_JARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3671f2a4-8fe4-422b-a437-537842e3a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.26\" 2025-01-21\n",
      "OpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu120.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu120.04, mixed mode, sharing)\n",
      "JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64\n",
      "Name: pyspark\n",
      "Version: 3.4.4\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /home/cvmi_jeyoung/anaconda3/envs/rapids2/lib/python3.10/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "SPARK_HOME: /media/HDD/JeYoung/Fundamentals_of_GPU_DataScience_with_RAPIDS/Chapter07/pyspark/spark-3.4.4-bin-hadoop3\n",
      "/home/cvmi_jeyoung/anaconda3/envs/rapids2/bin/python\n",
      "PYSPARK_PYTHON: python3\n"
     ]
    }
   ],
   "source": [
    "# 1. Java 확인\n",
    "!java -version\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "# 2. PySpark 확인\n",
    "!pip show pyspark\n",
    "\n",
    "# 3. Spark 홈 디렉토리 확인\n",
    "print(\"SPARK_HOME:\", os.environ.get(\"SPARK_HOME\"))\n",
    "\n",
    "# 4. Python 실행 경로 확인\n",
    "!which python\n",
    "print(\"PYSPARK_PYTHON:\", os.environ.get(\"PYSPARK_PYTHON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274dfa74-7c03-4ecc-8410-0c960a3b1fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  6 09:13:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 ...    Off |   00000000:05:00.0 Off |                  N/A |\n",
      "|  0%   47C    P8             13W /  250W |      14MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2080 ...    Off |   00000000:06:00.0  On |                  N/A |\n",
      "| 23%   52C    P8             24W /  250W |     955MiB /   8192MiB |      8%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1681      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A      2200      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A      1681      G   /usr/lib/xorg/Xorg                             59MiB |\n",
      "|    1   N/A  N/A      2200      G   /usr/lib/xorg/Xorg                            672MiB |\n",
      "|    1   N/A  N/A      2330      G   /usr/bin/gnome-shell                           46MiB |\n",
      "|    1   N/A  N/A      3973      G   ...71,262144 --variations-seed-version         96MiB |\n",
      "|    1   N/A  N/A     15901      G   ...erProcess --variations-seed-version         65MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c8d9d-3b7d-4a40-8858-24631ad89b34",
   "metadata": {},
   "source": [
    "## Set Optimized Pyspark RAPIDS Enviroments\n",
    "---\n",
    "You can configure PySpark settings through environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90487115-2104-4d4d-ac9b-bf725658a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    f'--jars {os.environ.get(\"SPARK_JARS\")} '\n",
    "    \"--master local[*] \"\n",
    "    \"--conf spark.plugins=com.nvidia.spark.SQLPlugin \"\n",
    "    \"--conf spark.rapids.sql.enabled=true \"\n",
    "    \"--conf spark.rapids.sql.explain=NONE \"  # GPU 실행 로그를 출력하도록 설정\n",
    "    \"--conf spark.rapids.memory.gpu.allocFraction=0.6 \"  # GPU 메모리 사용량 60% 설정\n",
    "    \"--conf spark.rapids.memory.gpu.maxAllocFraction=0.9 \"  # 최대 90%까지 사용 가능\n",
    "    \"pyspark-shell\"    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf020b13-1512-4d7f-b441-b62ef9842878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--jars /media/HDD/JeYoung/Fundamentals_of_GPU_DataScience_with_RAPIDS/Chapter07/pyspark/rapids-4-spark_2.12-24.12.0.jar --master local[*] --conf spark.plugins=com.nvidia.spark.SQLPlugin --conf spark.rapids.sql.enabled=true --conf spark.rapids.sql.explain=NONE --conf spark.rapids.memory.gpu.allocFraction=0.6 --conf spark.rapids.memory.gpu.maxAllocFraction=0.9 pyspark-shell'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"PYSPARK_SUBMIT_ARGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bdde04-1cf0-4ce6-8303-a198b68ac1b3",
   "metadata": {},
   "source": [
    "## Start jupyter notebook with plugin config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f309a920-2f8a-49f1-8fce-35b5bce91a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import cupy as cp\n",
    "import pyspark\n",
    "from pyspark import broadcast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Check Spark Version\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1787898e-b8b8-42dd-b8fd-2a996915205e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/06 09:13:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 09:13:52 WARN RapidsPluginUtils: RAPIDS Accelerator 24.12.0 using cudf 24.12.0, private revision 874fe2a69931879dc50faad111280e8ca75467cc\n",
      "25/03/06 09:13:52 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "25/03/06 09:14:02 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/06 09:14:02 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:600)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test GPU</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f43ce2a9510>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"Test GPU\")\n",
    "        .getOrCreate())\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f763b3b7-6193-4365-9b04-2a652f03fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|double_num|\n",
      "+----------+\n",
      "|         0|\n",
      "|         2|\n",
      "|         4|\n",
      "|         6|\n",
      "|         8|\n",
      "|        10|\n",
      "|        12|\n",
      "|        14|\n",
      "|        16|\n",
      "|        18|\n",
      "|        20|\n",
      "|        22|\n",
      "|        24|\n",
      "|        26|\n",
      "|        28|\n",
      "|        30|\n",
      "|        32|\n",
      "|        34|\n",
      "|        36|\n",
      "|        38|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10000).toDF(\"num\")\n",
    "df.selectExpr(\"num * 2 as double_num\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210cb500-8f85-47e0-8b9d-36edd6d56a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init spark\n",
    "spark.sparkContext.cancelAllJobs()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3dbd43-ebd6-499d-b981-99a7a4d58cfa",
   "metadata": {},
   "source": [
    "## Change Configuration on Codes\n",
    "---\n",
    "\n",
    " If you want to optimize your PySpark code, \n",
    " \n",
    " Try customizing the configuration in your code to improve performance.\n",
    "\n",
    " See The Configuration Doc [LINK](https://nvidia.github.io/spark-rapids/docs/configs.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d56345-4f14-464a-a9ee-85f017133a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/06 09:14:08 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local-cluster[2,8,8192]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test GPU</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f43ce2a91b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"Test GPU\")\n",
    "        .master(\"local-cluster[2,8,8192]\") # 2 Executor, 8 Core, 8192MB per Executor\n",
    "        .getOrCreate())\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b5fc55-5a89-4a4d-896a-86423e9f2b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|double_num|\n",
      "+----------+\n",
      "|         0|\n",
      "|         2|\n",
      "|         4|\n",
      "|         6|\n",
      "|         8|\n",
      "|        10|\n",
      "|        12|\n",
      "|        14|\n",
      "|        16|\n",
      "|        18|\n",
      "|        20|\n",
      "|        22|\n",
      "|        24|\n",
      "|        26|\n",
      "|        28|\n",
      "|        30|\n",
      "|        32|\n",
      "|        34|\n",
      "|        36|\n",
      "|        38|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.range(10000).toDF(\"num\")\n",
    "df.selectExpr(\"num * 2 as double_num\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58928366-59d5-429c-a11b-eba814decef1",
   "metadata": {},
   "source": [
    "If you want to see other option\n",
    "\n",
    "See [LINK](https://nvidia.github.io/spark-rapids/docs/additional-functionality/advanced_configs.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b1e6d02-6136-4641-bb44-638219788224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init spark\n",
    "spark.sparkContext.cancelAllJobs()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e056f882-5321-4915-a0a5-b1565c285ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/06 14:47:00 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "25/03/06 14:47:02 WARN GpuDeviceManager: The default cuDF host pool was already configured\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test GPU</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f43dbac9b10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"Test GPU\")\n",
    "         # .master(\"local-cluster[2,8,8192]\") # 2 Excutor, 4 Core, 2048MB         \n",
    "        # .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        # .config(\"spark.rapids.sql.enabled\", \"true\")\n",
    "         .config(\"spark.executor.resource.gpu.discoveryScript\", \"./sample_scripts/GPU_info.sh\")\n",
    "         .config(\"spark.executor.instances\", \"2\")  # Executor 개수 줄이기\n",
    "         .config(\"spark.executor.cores\", \"2\")\n",
    "         .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "        .config(\"spark.task.resource.gpu.amount\", \"0.5\") \n",
    "        # .config(\"spark.executor.resource.gpu.amount\", \"1\")   # Executor당 1개 GPU 사용\n",
    "         .config(\"spark.executorEnv.CUDA_VISIBLE_DEVICES\", \"0,1\") # For Multi-GPUs\n",
    "\n",
    "        # .config(\"spark.rapids.memory.gpu.allocFraction\", \"0.6\") \n",
    "        # .config(\"spark.rapids.memory.gpu.maxAllocFraction\", \"0.9\") \n",
    "         \n",
    "        # .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "        # .config(\"spark.jars\", os.environ.get('SPARK_JARS')) \n",
    "        .config(\"spark.rapids.sql.multiThreadedRead.numThreads\", \"10\")\n",
    "        .config('spark.rapids.sql.enabled','true')\n",
    "        .config('spark.rapids.sql.format.csv.read.enabled', 'true')\n",
    "        .config('spark.rapids.sql.format.csv.enabled', 'true')\n",
    "         # .config('spark.dynamicAllocation.enabled', 'false')\n",
    "        # .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "        .getOrCreate())\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b694a5-841c-4e31-9e4b-f4837465310f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]"
     ]
    }
   ],
   "source": [
    "df = spark.range(100000).toDF(\"num\")\n",
    "df.selectExpr(\"num * 2 as double_num\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046233b-26d0-4038-b1e0-f012169f3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init spark\n",
    "spark.sparkContext.cancelAllJobs()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91d9bc-5f07-43ee-b570-ed853cf8b3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"Test GPU\")\n",
    "         .master(\"local-cluster[2,8,8192]\") # 2 Excutor, 4 Core, 2048MB         \n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        # .config(\"spark.rapids.sql.enabled\", \"true\")\n",
    "        .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "        .config(\"spark.task.resource.gpu.amount\", \"0.25\") \n",
    "\n",
    "        # .config(\"spark.rapids.memory.gpu.allocFraction\", \"0.6\") \n",
    "        # .config(\"spark.rapids.memory.gpu.maxAllocFraction\", \"0.9\") \n",
    "         \n",
    "        # .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "        # .config(\"spark.jars\", os.environ.get('SPARK_JARS')) \n",
    "        # .config(\"spark.rapids.sql.multiThreadedRead.numThreads\", \"10\")\n",
    "        # .config(\"spark.driver.extraJavaOptions\", \"-Djava.library.path=/usr/lib/jvm/java-11-openjdk-amd64\")\n",
    "        # .config('spark.rapids.sql.enabled','true')\n",
    "        # .config('spark.rapids.sql.format.csv.read.enabled', 'true')\n",
    "        # .config('spark.rapids.sql.format.csv.enabled', 'true')\n",
    "         # .config('spark.dynamicAllocation.enabled', 'false')\n",
    "        # .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "        .getOrCreate())\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d1df2-b4e3-4b1e-b44b-85edfbc3df35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"spark.plugins:\", spark.conf.get(\"spark.plugins\"))\n",
    "print(\"spark.rapids.sql.enabled:\", spark.conf.get(\"spark.rapids.sql.enabled\"))\n",
    "# print(\"spark.executor.cores:\", spark.conf.get(\"spark.executor.cores\"))\n",
    "# print(\"spark.executor.resource.gpu.amount:\", spark.conf.get(\"spark.executor.resource.gpu.amount\"))\n",
    "# print(\"spark.task.resource.gpu.amount:\", spark.conf.get(\"spark.task.resource.gpu.amount\"))\n",
    "# print(\"SparkContext 상태:\", spark.sparkContext._jsc.sc().isStopped())\n",
    "# print(\"Spark Master URL:\", spark.conf.get(\"spark.master\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37222635-cfdd-40f1-afae-ef9693223fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.range(100).toDF(\"num\")\n",
    "df.cache()  # ✅ Spark에 데이터 캐싱 → Task 실행 유도\n",
    "df.selectExpr(\"num * 2 as double_num\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5fa3b-2c6e-45db-940f-f0905cd17ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init spark\n",
    "spark.sparkContext.cancelAllJobs()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a65a6-9225-4fde-8338-639a01636583",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc861aee-fade-4a01-bfd8-6c51b44b1650",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5075d4b-c47f-40b5-a889-04de827cb1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"TitanicETL\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"INFO\")  # 로그 레벨을 INFO로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01d7c2-3188-4c5c-ba8e-a9b8fb2cebf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"spark.plugins:\", spark.conf.get(\"spark.plugins\"))\n",
    "print(\"spark.rapids.sql.enabled:\", spark.conf.get(\"spark.rapids.sql.enabled\"))\n",
    "print(\"spark.executor.cores:\", spark.conf.get(\"spark.executor.cores\"))\n",
    "print(\"spark.executor.resource.gpu.amount:\", spark.conf.get(\"spark.executor.resource.gpu.amount\"))\n",
    "print(\"spark.task.resource.gpu.amount:\", spark.conf.get(\"spark.task.resource.gpu.amount\"))\n",
    "print(\"SparkContext 상태:\", spark.sparkContext._jsc.sc().isStopped())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ca8a2-e1c2-47cb-94c6-9e4e8ea4cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.uiWebUrl  # Spark UI 주소 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94883c6-279b-4b8a-bbac-c1a4e44fe289",
   "metadata": {},
   "source": [
    "## GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c773d-8ae8-4bde-9600-67f1239390e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = spark.conf.get(\"spark.executor.resource.gpu.amount\", \"Not Found\")\n",
    "print(f\"✅ Spark가 감지한 GPU 수: {gpu_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2f4e8-fdbd-47f1-8190-ef91bc24ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "_csv_raw_schema = StructType([\n",
    "    StructField('PassengerId', IntegerType()),\n",
    "    StructField(\"Survival\", IntegerType()),\n",
    "    StructField(\"Pclass\", IntegerType()),\n",
    "    StructField('Name', StringType()),\n",
    "    StructField(\"Sex\", StringType()),\n",
    "    StructField(\"Age\", DoubleType()),\n",
    "    StructField(\"SibSp\", IntegerType()),\n",
    "    StructField(\"Parch\", IntegerType()),\n",
    "    StructField(\"Ticket \", StringType()),\n",
    "    StructField(\"Fare\", DoubleType()),\n",
    "    StructField(\"Cabin\", StringType()),\n",
    "    StructField(\"Embarked\", StringType()),])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a971ec-ea93-49dd-a7f3-3efdbd487278",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_col_names = [\n",
    "        \"Name\",\n",
    "        \"Sex\",\n",
    "        \"Ticket\",\n",
    "        \"Cabin\",\n",
    "        \"Embarked\",\n",
    "]\n",
    "\n",
    "label_col_name = \"Survival\"\n",
    "numeric_col_names = [\n",
    "        \"PassengerId\",\n",
    "        \"Pclass\",\n",
    "        \"Age\",\n",
    "        \"SibSp\",\n",
    "        \"Parch\",\n",
    "        \"Fare\",\n",
    "        label_col_name\n",
    "]\n",
    "\n",
    "all_col_names = cate_col_names + numeric_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a5e7c-5642-40c5-9419-9aabb156cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_perf_columns(rawDf):\n",
    "    perfDf = rawDf.select(\n",
    "        col('Pclass'),\n",
    "        col('Sex'),\n",
    "        col('SibSp'),\n",
    "        col('Parch'),\n",
    "        col('Age'),\n",
    "        col('Embarked_l'),\n",
    "        col('Fare'),\n",
    "        col('Survival'),\n",
    "    )\n",
    "\n",
    "    return perfDf.select(\"*\")\n",
    "\n",
    "def read_raw_csv(spark, path):\n",
    "    return spark.read.format('csv') \\\n",
    "            .option('nullValue', cp.nan) \\\n",
    "            .option('delimiter', ',') \\\n",
    "            .schema(_csv_raw_schema) \\\n",
    "            .load(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041822f8-11ed-464b-9ff5-d213464c78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set('spark.rapids.sql.explain', 'ALL')\n",
    "# spark.conf.set('spark.rapids.sql.batchSizeBytes', '512M')\n",
    "# spark.conf.set('spark.rapids.sql.reader.batchSizeBytes', '768M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d8d55-4d15-4069-9fc0-32e4eb652bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_raw_csv(spark, '../data/titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2202be-6a00-463e-b2c2-2511327732c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9b64e-7ad8-4059-ba9b-fdffbadc4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930f6bb-2d13-4bb3-8c15-6154e3294ef6",
   "metadata": {},
   "source": [
    "## Too Simple\n",
    "---\n",
    "\n",
    "You can read on your titanic dataset on GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
